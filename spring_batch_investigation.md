Ótimo! Vou pesquisar as melhores opções de frameworks e estratégias para processamento eficiente desses arquivos grandes no S3 dentro de um ambiente EKS usando Java 17 e Spring Batch 5.1.1. Também vou avaliar as diferenças entre PostgreSQL e MongoDB para armazenar e processar os dados, levando em conta o volume alto de inserções diárias. Assim que tiver as informações, te aviso.

# Processamento massivo com Spring Batch, Spark/Flink e escolha de banco de dados

## Processando 20 milhões de linhas no S3 com Spring Batch no EKS

Para lidar com arquivos de ~20 milhões de linhas (vários GB) de forma eficiente, o **Spring Batch 5.1.1** oferece recursos robustos de processamento em lote. Ele utiliza o modelo *chunk-oriented* – lê dados em pedaços e grava em transações controladas – evitando carregar todo o arquivo em memória ([Methods for Efficient Large File Processing in Spring Boot | by Anh Trần Tuấn | tuanhdotnet | Medium](https://medium.com/tuanhdotnet/methods-for-efficient-large-file-processing-in-spring-boot-5ed58f36377a#:~:text=1)) ([Methods for Efficient Large File Processing in Spring Boot | by Anh Trần Tuấn | tuanhdotnet | Medium](https://medium.com/tuanhdotnet/methods-for-efficient-large-file-processing-in-spring-boot-5ed58f36377a#:~:text=OutputStream%20classes%20are%20used%20to,be%20used%20to%20achieve%20this)). Em vez disso, o arquivo S3 pode ser lido sequencialmente via streaming (por exemplo, usando `InputStream` do S3) e processado em chunks, prevenindo estouro de memória e melhorando a performance em arquivos grandes ([Methods for Efficient Large File Processing in Spring Boot | by Anh Trần Tuấn | tuanhdotnet | Medium](https://medium.com/tuanhdotnet/methods-for-efficient-large-file-processing-in-spring-boot-5ed58f36377a#:~:text=Streaming%20is%20one%20of%20the,larger%20than%20the%20available%20RAM)) ([Methods for Efficient Large File Processing in Spring Boot | by Anh Trần Tuấn | tuanhdotnet | Medium](https://medium.com/tuanhdotnet/methods-for-efficient-large-file-processing-in-spring-boot-5ed58f36377a#:~:text=1)). 

Para acelerar, é essencial configurar um **chunk size** adequado (por exemplo, centenas ou milhares de registros por transação) e usar escrita em lote no banco de dados (o `JdbcBatchItemWriter` já faz commits em lote). Além disso, o Spring Batch suporta **processamento paralelo** de várias formas ([Scaling and Parallel Processing :: Spring Batch](https://docs.spring.io/spring-batch/reference/scalability.html#:~:text=These%20break%20down%20into%20categories,as%20well%2C%20as%20follows)). A opção mais simples é um *Step multi-threaded*: adicionar um `TaskExecutor` no Step para que múltiplos threads leiam/processem linhas simultaneamente ([Scaling and Parallel Processing :: Spring Batch](https://docs.spring.io/spring-batch/reference/scalability.html#:~:text=%2A%20Multi)) ([Methods for Efficient Large File Processing in Spring Boot | by Anh Trần Tuấn | tuanhdotnet | Medium](https://medium.com/tuanhdotnet/methods-for-efficient-large-file-processing-in-spring-boot-5ed58f36377a#:~:text=offers%20features%20such%20as%20chunk,jobs%20like%20large%20file%20processing)). Outra opção é particionar o trabalho – por exemplo, dividir o arquivo em partes lógicas e processar cada parte em um Step/pod separado (usando *partitioning* ou *remote chunking*, aproveitando o ambiente Kubernetes). A documentação destaca que muitos cenários podem ser resolvidos até mesmo com uma única instância e I/O eficiente (ler/escrever algumas centenas de MB por minuto é viável em hardware padrão) ([Scaling and Parallel Processing :: Spring Batch](https://docs.spring.io/spring-batch/reference/scalability.html#:~:text=Many%20batch%20processing%20problems%20can,minute%2C%20even%20with%20standard%20hardware)), mas com 20 milhões de linhas provavelmente será necessário escalar horizontalmente.

No EKS (Kubernetes), pode-se usar **Jobs do Kubernetes** para executar o batch container. Cada Job Spring Batch pode ser configurado para ler um determinado segmento do arquivo S3 (por exemplo, usando *offsets* ou divisões prévias do arquivo). Há arquiteturas que usam o **AWS Step Functions** para orquestrar a divisão do arquivo S3 e disparar múltiplos Jobs Kubernetes em paralelo para processar cada parte ([Batch processing in scale using Kubernetes jobs | by Hari Ohm Prasath | Geek Culture | Medium](https://medium.com/geekculture/batch-processing-in-scale-using-kubernetes-jobs-2759fdf9bdb3#:~:text=%2A%20Sequential%20read%20%28aka%20single,individual%20messages%20to%20a%20message)). Essa abordagem paraleliza a ingestão e reduz drasticamente o tempo total de processamento comparado a uma leitura sequencial única ([Batch processing in scale using Kubernetes jobs | by Hari Ohm Prasath | Geek Culture | Medium](https://medium.com/geekculture/batch-processing-in-scale-using-kubernetes-jobs-2759fdf9bdb3#:~:text=%2A%20Sequential%20read%20%28aka%20single,individual%20messages%20to%20a%20message)). Em resumo, dentro do EKS você pode escalar o Spring Batch tanto verticalmente (threads) quanto horizontalmente (múltiplos pods) para atingir o throughput necessário.

 ([Spring Batch - Architecture](https://www.tutorialspoint.com/spring_batch/spring_batch_architecture.htm)) *Componentes principais de um job Spring Batch: um *Job* contém um ou mais *Steps*, cada *Step* normalmente usando leitor/processador/gravador em chunk. Essa arquitetura suporta transações e reinício seguro de jobs em caso de falhas.* ([Methods for Efficient Large File Processing in Spring Boot | by Anh Trần Tuấn | tuanhdotnet | Medium](https://medium.com/tuanhdotnet/methods-for-efficient-large-file-processing-in-spring-boot-5ed58f36377a#:~:text=offers%20features%20such%20as%20chunk,jobs%20like%20large%20file%20processing))

## Apache Spark vs Apache Flink vs Spring Cloud Data Flow (Kafka Integration)

Para processamento de dados em larga escala e integração com **Kafka**, existem frameworks especializados que podem complementar ou substituir o Spring Batch dependendo do caso de uso:

### Apache Spark

**Apache Spark** é um mecanismo de processamento distribuído amplamente usado para big data e machine learning. Ele foi historicamente voltado a *batch processing*, mas suporta streams via **Spark Structured Streaming** (micro-batching sobre fontes como Kafka) ([Apache Spark vs. Apache Flink: A Comparison of the Data ... - Arbisoft](https://arbisoft.com/blogs/apache-spark-vs-apache-flink-a-comparison-of-the-data-processing-duo#:~:text=Explore%20the%20key%20differences%2C%20applications%2C,Flink%E2%80%94two%20leading%20data%20processing%20frameworks)). Em termos de eficiência, Spark brilha em jobs de lote de altíssimo volume, distribuindo o processamento em um cluster e mantendo dados em memória para acelerar etapas repetitivas. Por exemplo, é comum usar Spark para ler grandes arquivos ou tabelas, transformar os dados em paralelo e carregar resultados em um data store. O Spark integra-se nativamente ao Kafka – é capaz de ler de tópicos Kafka e gravar em tópicos ou outras saídas usando a API de streaming estruturado ([Apache Spark vs Spring Cloud data flow - Stack Overflow](https://stackoverflow.com/questions/51452952/apache-spark-vs-spring-cloud-data-flow#:~:text=might%20use%20Spring%20Batch,applications%20to%20support%20Stream%20processing)). Isso facilita construir pipelines onde o Kafka alimenta o processamento em micro-lotes contínuos. No contexto do EKS, é possível rodar Spark em contêineres (Spark-on-Kubernetes), porém deve-se considerar o overhead de manter um cluster Spark dentro do Kubernetes. Para um arquivo de 20 milhões de linhas diário, o Spark poderia ser um *overkill* se usado apenas em batch, mas se a arquitetura evoluir para *streams* contínuos e análise distribuída, Spark oferece alto throughput. Ele é bastante maduro, com um ecossistema rico (SQL, ML, Grafos) e ampla adoção, o que facilita encontrar suporte e recursos ([Apache Spark vs Flink, a detailed comparison - Macrometa](https://www.macrometa.com/event-stream-processing/spark-vs-flink#:~:text=Apache%20Spark%20vs%20Flink%2C%20a,easy%20to%20find%20many)). Contudo, para *stream processing* de baixa latência, Spark Structured Streaming tem latências maiores (dezenas de segundos) devido ao modelo de micro-batch, então não é a escolha ideal se a necessidade for reação em tempo quase real.

### Apache Flink

**Apache Flink** é uma plataforma de processamento distribuído desenhada com foco em *streaming de baixa latência*. Diferentemente do Spark, o Flink foi *stream-first* e trata batch como um caso especial de stream delimitado ([A side-by-side comparison of Apache Spark and Apache Flink for common streaming use cases | AWS Big Data Blog](https://aws.amazon.com/blogs/big-data/a-side-by-side-comparison-of-apache-spark-and-apache-flink-for-common-streaming-use-cases/#:~:text=Apache%20Flink%20and%20Apache%20Spark,first%20framework)) ([A side-by-side comparison of Apache Spark and Apache Flink for common streaming use cases | AWS Big Data Blog](https://aws.amazon.com/blogs/big-data/a-side-by-side-comparison-of-apache-spark-and-apache-flink-for-common-streaming-use-cases/#:~:text=its%20ease%20of%20use%2C%20high,first%20framework)). Em outras palavras, Flink consegue processar dados em tempo real com latência muito menor, suportando processamento evento-a-evento e gerenciamento de estado eficiente para computações com estado (contagens, janelas, etc.). Ele também possui conectores nativos para Kafka – permite ler e escrever em Kafka com garantia de **exactly-once** ponta a ponta em muitos cenários ([Kafka | Apache Flink](https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/kafka/#:~:text=Flink%20provides%20an%20Apache%20Kafka,once%20guarantees)). Isso o torna excelente para pipelines em que dados chegam continuamente via Kafka e precisam ser processados com alta consistência. Em termos de performance, o Flink costuma ter vantagem em cenários de streaming puro (onde a consistência de tempo e baixa latência importam), conseguindo throughput alto e reagindo a picos com mecanismo de *backpressure*. Para processar 20 milhões de eventos diários, o Flink pode facilmente acompanhar em tempo real se os eventos fluírem via Kafka em vez de um arquivo único. No caso de ler diretamente de S3, o Flink também pode executar um job batch, mas seu maior benefício surge se integrarmos com fluxos contínuos. A curva de aprendizado do Flink pode ser um pouco mais íngreme que Spark, mas ele é extremamente poderoso para *stream processing* contínuo com requisitos de latência baixa e processamento com estado complexo (por exemplo, detecção de padrões, janelas deslizantes baseadas em tempo de evento, etc.). Em resumo, se a tarefa evoluir para uma ingestão contínua (via Kafka) de volumes altos, Flink oferece eficiência e escalabilidade excelentes. Caso seja apenas um processo diário isolado, o ganho em usar Flink vs Spark pode não ser tão significativo, mas o Flink ainda poderia lidar bem com o batch como um stream delimitado.

### Spring Cloud Data Flow

**Spring Cloud Data Flow (SCDF)** é uma plataforma de orquestração de pipelines de dados baseada em microserviços Spring. Diferentemente do Spark e Flink (motores de processamento), o SCDF não executa todo o processamento dentro de um único cluster engine, mas sim coordena aplicações Spring Boot independentes que se comunicam via mensagens. Ele suporta tanto pipelines de *streaming* quanto tarefas *batch* ([Spring Cloud Data Flow: Benefits & Usage | Medium](https://medium.com/@AlexanderObregon/understanding-the-concept-of-spring-cloud-data-flow-and-its-benefits-72311d16106#:~:text=What%20is%20Spring%20Cloud%20Data,Flow)) ([Spring Cloud Data Flow: Benefits & Usage | Medium](https://medium.com/@AlexanderObregon/understanding-the-concept-of-spring-cloud-data-flow-and-its-benefits-72311d16106#:~:text=3)). Por exemplo, um pipeline típico de streaming no SCDF pode usar **Spring Cloud Stream** com binder Kafka: cada etapa (source, processor, sink) é um microserviço Spring Boot conectado a um tópico Kafka. O SCDF gerencia o deploy dessas apps (no Kubernetes, Cloud Foundry, etc.), a escala de instâncias e a conexão entre elas. Para integração com Kafka, o SCDF se encaixa naturalmente – ele usa o Kafka (ou RabbitMQ) como backbone de comunicação dos streams ([Apache Spark vs Spring Cloud data flow - Stack Overflow](https://stackoverflow.com/questions/51452952/apache-spark-vs-spring-cloud-data-flow#:~:text=)). Isso permite facilmente ler de tópicos Kafka, aplicar processamentos customizados em Java (usando familiaridade do Spring ecosystem), e enviar a saída para outro tópico ou banco. Em termos de desempenho, um pipeline SCDF escalado pode atingir altos throughputs, mas ele depende do Kafka e do escalonamento manual das instâncias para paralelismo (p. ex., configurar múltiplas instâncias de um processor para consumir diferentes partitions do tópico Kafka). Não terá, por exemplo, um otimizador global como Spark, mas em compensação cada componente é isolado e pode ser desenvolvido, versionado e escalado independentemente – seguindo princípios de microserviços ([Spring Cloud Data Flow: Benefits & Usage | Medium](https://medium.com/@AlexanderObregon/understanding-the-concept-of-spring-cloud-data-flow-and-its-benefits-72311d16106#:~:text=Spring%20Cloud%20Data%20Flow%20is,Foundry%2C%20Apache%20Mesos%2C%20or%20Kubernetes)) ([Spring Cloud Data Flow: Benefits & Usage | Medium](https://medium.com/@AlexanderObregon/understanding-the-concept-of-spring-cloud-data-flow-and-its-benefits-72311d16106#:~:text=1.%20Microservices)). Para o caso em questão, poderíamos ter um stream onde uma fonte lê o arquivo do S3 e envia registros para Kafka, e um ou mais processadores consumem do Kafka para validar/enriquecer e então gravar em banco de dados. O Spring Cloud Data Flow orquestraria essa topologia e forneceria monitoramento. Além disso, o SCDF também pode orquestrar **tarefas batch** (por exemplo, disparar jobs Spring Batch como tasks), o que significa que você poderia usá-lo para agendar e gerenciar o Spring Batch que lê do S3 – unificando tanto fluxos em tempo real quanto cargas batch na mesma plataforma. Em suma, o SCDF é ideal se você quer uma solução Spring nativa para pipelines heterogêneos, com forte integração ao Kafka e que aproveite a familiaridade do ecossistema Spring. Ele pode não ser tão raw-performance quanto um Spark ou Flink para computações intensivas, mas oferece **flexibilidade e facilidade de composição** de pipelines de streaming e batch, além de escalabilidade horizontal via instâncias adicionais dos microserviços.

## PostgreSQL vs MongoDB para armazenar 20 milhões de registros diários

Ao armazenar ~20 milhões de novos registros por dia com consultas rápidas por ID, a escolha entre **PostgreSQL** e **MongoDB** deve considerar escalabilidade de escrita, desempenho de leitura por chave e crescimento de dados ao longo do tempo.

**PostgreSQL** é um banco de dados relacional ACID maduro, capaz de lidar com volumes muito altos quando bem configurado. Várias empresas utilizam Postgres com bilhões de registros – o próprio Yahoo e Reddit, por exemplo, usam em cenários massivos ([postgresql - Postgres performance for a table with more than Billion rows - Stack Overflow](https://stackoverflow.com/questions/45629767/postgres-performance-for-a-table-with-more-than-billion-rows#:~:text=6)). O consenso é que o Postgres em si não será o gargalo, mas sim a configuração de hardware e otimizações de banco que você aplicar ([postgresql - Postgres performance for a table with more than Billion rows - Stack Overflow](https://stackoverflow.com/questions/45629767/postgres-performance-for-a-table-with-more-than-billion-rows#:~:text=6)) ([postgresql - Postgres performance for a table with more than Billion rows - Stack Overflow](https://stackoverflow.com/questions/45629767/postgres-performance-for-a-table-with-more-than-billion-rows#:~:text=From%20the%20perspective%20of%20open,database%20design%20and%20hardware%20configuration)). Para cargas de inserção grandes, Postgres pode atingir throughput elevado usando *COPY* ou batches de inserts, especialmente se reduzir índices ou constraints desnecessárias durante a carga. Um ponto crucial é adotar **particionamento de tabelas** para volumes diários tão altos. Com 20 milhões/dia, em um mês seriam ~600 milhões de linhas; em um ano, muitos bilhões. O Postgres permite particionar por faixa (por exemplo, por data) ([postgresql - Postgres performance for a table with more than Billion rows - Stack Overflow](https://stackoverflow.com/questions/45629767/postgres-performance-for-a-table-with-more-than-billion-rows#:~:text=design%20and%20hardware%20configuration)) ([postgresql - Postgres performance for a table with more than Billion rows - Stack Overflow](https://stackoverflow.com/questions/45629767/postgres-performance-for-a-table-with-more-than-billion-rows#:~:text=id%20%20%20%20,date%2C)), o que ajuda a distribuir a carga e facilitar manutenções (como descartar dados antigos, se aplicável). Consultas por ID (chave primária) são rápidas desde que haja índice – o índice B-tree do Postgres buscará o registro em O(log n). Porém, conforme a tabela cresce, o índice também cresce; para manter leituras rápidas, é importante ter memória suficiente para cache de páginas quentes e talvez usar particionamento de índice (por exemplo, se o ID incorpora data, direcionar a consulta à partição certa). Em termos de escala, há relatos de uso de Postgres com dezenas de milhões de inserções diárias em bases de vários terabytes sem problemas ([sql - How big is too big for a PostgreSQL table? - Stack Overflow](https://stackoverflow.com/questions/21866113/how-big-is-too-big-for-a-postgresql-table#:~:text=30)) ([sql - How big is too big for a PostgreSQL table? - Stack Overflow](https://stackoverflow.com/questions/21866113/how-big-is-too-big-for-a-postgresql-table#:~:text=Rows%20per%20a%20table%20won%27t,an%20issue%20on%20it%27s%20own)). Ajustes de parâmetros (como `shared_buffers`, `maintenance_work_mem`, etc.) e uso de replicação ou sharding (com extensões como Citus) também podem aumentar a capacidade. Benchmarks indicam que o Postgres frequentemente supera o MongoDB em throughput e latência quando ambos são comparados em condições equivalentes (mesmo nível de consistência) ([](https://info.enterprisedb.com/rs/069-ALB-339/images/PostgreSQL_MongoDB_Benchmark-WhitepaperFinal.pdf#:~:text=has%20been%20tested%20both%20with,MongoDB%20doesn%E2%80%99t%20reach%202%20thousand)). Ou seja, em leituras simples por chave e escritas, o Postgres tende a ser altamente eficiente, desde que não esteja sobrecarregado por *joins* complexos ou transações de longa duração nesse cenário de uso.

**MongoDB**, por sua vez, é um banco de dados NoSQL orientado a documentos, projetado para escalabilidade horizontal e esquemas flexíveis. Uma vantagem do MongoDB é a facilidade de **sharding** nativo – ou seja, você pode distribuir coleções de documentos em vários nós (shards) para dividir a carga de escrita e o volume de dados. Com 20 milhões de documentos diários, um cluster MongoDB pode ser dimensionado adicionando shards e mantendo cada nó responsável por um subconjunto dos dados (por exemplo, shard key por faixa de IDs ou por data). Consultas por ID no MongoDB são atendidas via índice em chave (o `_id` padrão ou outro definido), de forma similar ao Postgres – retornando tipicamente em milissegundos enquanto o índice cabe em RAM. MongoDB é conhecido por suportar altas taxas de operações: em um caso prático, atingiu ~500 milhões de operações por dia (~5.8k por segundo) com latência média ~10ms por operação ([mongoDB vs PostgreSQL : r/node](https://www.reddit.com/r/node/comments/1cxueir/mongodb_vs_postgresql/#:~:text=We%20use%20both,reporting%20off%20the%20Mongo%20data)). Isso demonstra que para escrita e leitura simples, ele pode escalar bem. No entanto, há pontos de atenção: conforme uma coleção cresce para centenas de milhões ou bilhões de documentos, começam a surgir desafios de **manutenção e performance**. Usuários relatam que acima de ~100 milhões de documentos numa única coleção, o cluster MongoDB pode ficar “frágil” – exigindo mais memória para índices, operações de compactação, etc., e custando caro para manter a performance ([mongoDB vs PostgreSQL : r/node](https://www.reddit.com/r/node/comments/1cxueir/mongodb_vs_postgresql/#:~:text=super%20easy,as%20the%20db%20gets%20bigger)). Em outras palavras, para sustentar desempenho o MongoDB muitas vezes demanda investir em RAM abundante (para caber índices) e adicionar nós (o que aumenta custo). Além disso, se forem necessárias consultas mais complexas no futuro (aggregations, filtros variados), o MongoDB pode não oferecer a mesma rapidez ou flexibilidade de um SQL otimizado. Por outro lado, se o esquema de dados for flexível ou houver necessidade de armazenar JSONs complexos, o MongoDB leva vantagem por não precisar de migrações de esquema.

**Comparativo:** Se o objetivo principal é inserção massiva e consulta por ID, ambas as opções podem atender. O Postgres, surpreendentemente para alguns, costuma ter desempenho equiparável ou superior em cenários puramente transacionais e de key-value quando bem tunado ([](https://info.enterprisedb.com/rs/069-ALB-339/images/PostgreSQL_MongoDB_Benchmark-WhitepaperFinal.pdf#:~:text=has%20been%20tested%20both%20with,MongoDB%20doesn%E2%80%99t%20reach%202%20thousand)). Ele traz ainda a possibilidade de usar todo o poder SQL para futuras análises, e forte consistência sem configurações adicionais. No entanto, para lidar com o crescimento de dados no Postgres, provavelmente será necessário implementar **particionamento** e possivelmente rodar em máquinas robustas ou usar soluções de distribuição (p. ex. citus) caso uma única máquina não dê conta. Já o MongoDB proporciona uma escalabilidade horizontal mais *out-of-the-box* via sharding e pode simplificar o pipeline se você já estiver usando JSON como formato de dados. Em termos de consultas por ID, ambos são O(1) ou O(log n) via índice – a diferença estará na latência de disco vs memória cache. Uma consideração é o *coste total de propriedade*: clusters Mongo em larga escala tendem a custar mais do que um setup equivalente de Postgres (como observado em um relato onde o cluster MongoDB saiu mais caro que todos os serviços AWS somados, inclusive o RDS Postgres) ([mongoDB vs PostgreSQL : r/node](https://www.reddit.com/r/node/comments/1cxueir/mongodb_vs_postgresql/#:~:text=So%20we%20use%20Postgres%20for,is%20much%20easier%20with%20SQL)) ([mongoDB vs PostgreSQL : r/node](https://www.reddit.com/r/node/comments/1cxueir/mongodb_vs_postgresql/#:~:text=Both%20databases%20have%20more%20or,put%20together%20each%20month)). 

**Resumo:** Se você preza consistência, deseja aproveitar SQL e planeja eventualmente fazer queries mais elaboradas sobre os dados, o **PostgreSQL** seria a escolha recomendada – desde que você prepare estratégias de partição e infraestrutura para acomodar o volume crescente. O Postgres pode manusear 20 milhões diários e consultas rápidas por ID sem problemas conhecidos, conforme práticas já estabelecidas (um caso citou 30 milhões/dia em base de 5TB funcionando bem) ([sql - How big is too big for a PostgreSQL table? - Stack Overflow](https://stackoverflow.com/questions/21866113/how-big-is-too-big-for-a-postgresql-table#:~:text=30)). Por outro lado, se a prioridade for escalar horizontalmente de forma transparente e lidar com dados semi-estruturados, o **MongoDB** pode ser considerado – mas esteja ciente da necessidade de monitorar o tamanho das coleções, adicionar shards com o crescimento e investir em memória/índices para manter as consultas por ID rápidas. Em ambos os casos, será fundamental indexar corretamente o campo de ID (em MongoDB, normalmente o `_id` já é indexado por padrão) e acompanhar a taxa de inserções vs capacidade do cluster. Em suma, Postgres tende a oferecer **consultas mais rápidas** e desempenho consistente até volumes muito altos (quando bem configurado) ([](https://info.enterprisedb.com/rs/069-ALB-339/images/PostgreSQL_MongoDB_Benchmark-WhitepaperFinal.pdf#:~:text=has%20been%20tested%20both%20with,MongoDB%20doesn%E2%80%99t%20reach%202%20thousand)) ([postgresql - Postgres performance for a table with more than Billion rows - Stack Overflow](https://stackoverflow.com/questions/45629767/postgres-performance-for-a-table-with-more-than-billion-rows#:~:text=From%20the%20perspective%20of%20open,database%20design%20and%20hardware%20configuration)), enquanto MongoDB oferece **escala horizontal e flexibilidade**, ao custo de possivelmente exigir mais nós para sustentar o ritmo de 20 milhões de inserts/dia a longo prazo. Use casos puramente de chave-valor e altíssima escala também poderiam considerar alternativas como bancos NoSQL distribuídos (Cassandra, DynamoDB), mas entre Postgres e MongoDB, ambos podem ser feitos para atender a demanda – com vantagem do Postgres em desempenho bruto e do MongoDB em escalabilidade horizontal fácil. 

**Referências:** Spring Batch docs ([Scaling and Parallel Processing :: Spring Batch](https://docs.spring.io/spring-batch/reference/scalability.html#:~:text=Many%20batch%20processing%20problems%20can,minute%2C%20even%20with%20standard%20hardware)) ([Scaling and Parallel Processing :: Spring Batch](https://docs.spring.io/spring-batch/reference/scalability.html#:~:text=These%20break%20down%20into%20categories,as%20well%2C%20as%20follows)); Stack Overflow e blogs sobre Spark vs Flink ([Apache Spark vs Spring Cloud data flow - Stack Overflow](https://stackoverflow.com/questions/51452952/apache-spark-vs-spring-cloud-data-flow#:~:text=might%20use%20Spring%20Batch,applications%20to%20support%20Stream%20processing)) ([Kafka | Apache Flink](https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/kafka/#:~:text=Flink%20provides%20an%20Apache%20Kafka,once%20guarantees)); Medium/Reddit discussões sobre Postgres vs Mongo ([postgresql - Postgres performance for a table with more than Billion rows - Stack Overflow](https://stackoverflow.com/questions/45629767/postgres-performance-for-a-table-with-more-than-billion-rows#:~:text=design%20and%20hardware%20configuration)) ([mongoDB vs PostgreSQL : r/node](https://www.reddit.com/r/node/comments/1cxueir/mongodb_vs_postgresql/#:~:text=We%20use%20both,reporting%20off%20the%20Mongo%20data)), entre outros.
